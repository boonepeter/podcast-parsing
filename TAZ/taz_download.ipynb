{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping transcripts from Google Docs\n",
    "\n",
    "[The Adventure Zone](https://www.maximumfun.org/shows/adventure-zone) is a Dungeons and Dragons podcast that is enjoyable even to those who have never played DnD (like me). This notebook lays out how I downloaded the transcripts for some language analysis. Huge thanks to the people behind the volunteer [transcribing project](http://tazscripts.tumblr.com/). It's a great resource for anyone in the Deaf community or people like me who want to download all of the words that have been spoken to process.\n",
    "\n",
    "#### Written in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transripts are all hosted on Google docs for community editing. The [master tracking document](https://docs.google.com/document/d/1zVGa1zgr7BjMC_0L06_XpmuIn2jgDKHvPZkohHy0YrA/edit) has links to every transcript and what percent is completed. Scraping a google doc is a little harder than a web page. I copied the tracking document to my own google drive and published it to the web so that I could scrape all of the transcript links from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get html, find all table rows 'tr'\n",
    "tracking_doc_url = \"https://docs.google.com/document/d/e/2PACX-1vT5IOvuFz988qAbP2P5ELDIekf4mUdfBsF1LGGW8CxMcry0YjJ34H1iFYN1qx0B7eYPUzw4CiSd8kbM/pub\"\n",
    "tracking_html = requests.get(tracking_doc_url)\n",
    "tracking_beautiful = BeautifulSoup(tracking_html.text, \"lxml\")\n",
    "rows = tracking_beautiful.findAll('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_links = {}\n",
    "for row in rows:\n",
    "    #strip row.text because there is some trailing whitespace\n",
    "    if row.text.strip().endswith('100%'):\n",
    "        a = row.find('a')\n",
    "        completed_links[a.text] = a['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to pull the Google doc ID out of every link and request an auto download version of the Google doc (see [this site](http://jessicastansberry.com/googledrive/) for info on that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_doc_download(docID, download_as='txt'):\n",
    "    return 'https://docs.google.com/document/d/' + docID + '/export?format=' + download_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop will take a while to download all of the episodes\n",
    "os.mkdir('data')\n",
    "for episode, link in completed_links.items():\n",
    "    \n",
    "    try:\n",
    "        #regex search to pull the ID out of the link\n",
    "        docID = re.search(string=link, pattern=r'(?:/d/|id%3D)(.*?)(?:/|&)')[1]\n",
    "    except:\n",
    "        print(f\"{episode} didn't download ({link})\")\n",
    "        continue\n",
    "        \n",
    "    download_link = google_doc_download(docID, download_as='txt')\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(), 'data', episode + '.txt'), 'w') as openfile:\n",
    "        document = requests.get(download_link)\n",
    "        openfile.write(document.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that all of the files are downloaded we can parse them into csv files\n",
    "Loop through all of the downloaded text files and read them into a pandas DataFrame. The speaker and line can be split using a colon as a delimiter. More than one colon is present in some lines and not in others, so this has to be done after the file is loaded in. After expanding the DataFrame on colons, the lines can be put back together. I saved every file to a separate csv to hang onto them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = os.listdir(os.path.join(os.getcwd(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('export')\n",
    "for script in docs:\n",
    "    with open(os.path.join(os.getcwd(), 'data', script)) as myfile:\n",
    "        onefile = pd.read_csv(myfile, sep='\\n', header=None)\n",
    "    \n",
    "    #split on colon\n",
    "    onefile = pd.DataFrame(onefile[0].str.split(':', expand=True))\n",
    "    onefile = onefile.fillna('')\n",
    "    #piece line columns back together\n",
    "    onefile.iloc[:,1] = onefile.iloc[:,1].astype(str).str.cat(onefile.iloc[:,2:])\n",
    "    onefile = onefile.drop(columns=[i for i in range(2, len(onefile.columns))])\n",
    "    #see if the episode number can be parsed from the episode title\n",
    "    try:\n",
    "        epnum = int(re.search(string=script, pattern=r'[\\d]+')[0])\n",
    "    except:\n",
    "        #if not use the script name (without '.txt')\n",
    "        epnum = script[:-4]\n",
    "    \n",
    "    onefile['episode'] = epnum\n",
    "    #set column names\n",
    "    onefile.columns = ['speaker', 'lines', 'episode']\n",
    "    #reorder\n",
    "    onefile = onefile[['episode', 'speaker', 'lines']]\n",
    "    onefile.to_csv(os.path.join(os.getcwd(), 'export', f'{epnum}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the files in, add to a DataFrame, and save the combined csv file as \"TAZ.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = os.listdir(os.path.join(os.getcwd(), 'export'))\n",
    "\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), 'export', csvs[0]))\n",
    "df.columns = ['line_num', 'episode', 'speaker', 'lines']\n",
    "for csv in csvs[1:]:\n",
    "    newdf = pd.read_csv(os.path.join(os.getcwd(), 'export', csv))\n",
    "    newdf.columns = ['line_num', 'episode', 'speaker', 'lines']\n",
    "    df = df.append(newdf)\n",
    "\n",
    "df = df.sort_values(by=['episode', 'line_num'], ascending=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.line_num = df.line_num.astype(int)\n",
    "df.to_csv(os.path.join(os.getcwd(), 'TAZ.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scripts are not very clean. The headers of each transcript are still there. There are things in the speaker column like \\[laughing] or \\[Theme music]. But it is a good stoping point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
